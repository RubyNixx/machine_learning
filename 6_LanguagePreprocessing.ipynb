{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubyNixx/machine_learning/blob/main/6_LanguagePreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcMxtjZcsAe2"
      },
      "source": [
        "ht### Basic Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhdWwSySsGKn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtByJpV0sGHp"
      },
      "outputs": [],
      "source": [
        "# Just a simple test\n",
        "sentences = [\n",
        "    \"I like eggs and ham.\",\n",
        "    \"I love chocolate and bunnies.\",\n",
        "    \"I hate onions and ham.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k1197xjsGFF",
        "outputId": "e7ae2e35-dfb5-4197-9d5b-3dd966783d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like eggs and ham.\n",
            "[1, 4, 5, 2, 3]\n",
            "I love chocolate and bunnies.\n",
            "[1, 6, 7, 2, 8]\n",
            "I hate onions and ham.\n",
            "[1, 9, 10, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "MAX_VOCAB_SIZE = 20000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(sentences[0])\n",
        "print(sequences[0])\n",
        "print(sentences[1])\n",
        "print(sequences[1])\n",
        "print(sentences[2])\n",
        "print(sequences[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9bFIVevsGCL",
        "outputId": "f8207a0c-aec1-4942-d95a-84f6ebc46884"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'i': 1,\n",
              " 'and': 2,\n",
              " 'ham': 3,\n",
              " 'like': 4,\n",
              " 'eggs': 5,\n",
              " 'love': 6,\n",
              " 'chocolate': 7,\n",
              " 'bunnies': 8,\n",
              " 'hate': 9,\n",
              " 'onions': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# How to get the word to index mapping?\n",
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYvMq3wSsF_S",
        "outputId": "17425812-9a2f-42b9-b033-09c89c25aeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  4  5  2  3]\n",
            " [ 1  6  7  2  8]\n",
            " [ 1  9 10  2  3]]\n"
          ]
        }
      ],
      "source": [
        "# use the defaults\n",
        "data = pad_sequences(sequences)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNo7z8YWsF8n",
        "outputId": "a5e365d8-0b14-4963-e170-41cb67152cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  4  5  2  3]\n",
            " [ 1  6  7  2  8]\n",
            " [ 1  9 10  2  3]]\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQUENCE_LENGTH = 5\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq-iW-cysFsT",
        "outputId": "a0bcf403-53f7-4ef7-dda1-6d4ff0c4aba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  4  5  2  3  0  0  0  0  0]\n",
            " [ 1  6  7  2  8  0  0  0  0  0]\n",
            " [ 1  9 10  2  3  0  0  0  0  0]]\n"
          ]
        }
      ],
      "source": [
        "data = pad_sequences(sequences, maxlen=10, padding='post')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbwHPmW-tA3i",
        "outputId": "1af041a3-9300-42d7-e525-d0c9b7d1a482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  4  5  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  6  7  2  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  9 10  2  3]]\n"
          ]
        }
      ],
      "source": [
        "# too much padding\n",
        "data = pad_sequences(sequences, maxlen=20)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMhjPkZGtAwL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRGB4aDQm4TS"
      },
      "source": [
        "### Examples of stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUfaVQM0lRRE",
        "outputId": "493840b2-cc12-4783-afd9-d2fe54465d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 1: house\n",
            "Word 2: cat\n",
            "Word 3: caterpillar\n",
            "Word 4: hungry\n",
            "Word 5: fiddle\n"
          ]
        }
      ],
      "source": [
        "word1 = input('Word 1: ')\n",
        "word2 = input('Word 2: ')\n",
        "word3 = input('Word 3: ')\n",
        "word4 = input('Word 4: ')\n",
        "word5 = input('Word 5: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_kr4kqEllNr"
      },
      "outputs": [],
      "source": [
        "words = [word1, word2, word3, word4, word5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0m5vjarrGM"
      },
      "source": [
        "#### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1EK555nk55E",
        "outputId": "f35925d7-6b18-4f7b-9652-a7cdba5d3369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "house --> hous\n",
            "cat --> cat\n",
            "caterpillar --> caterpillar\n",
            "hungry --> hungri\n",
            "fiddle --> fiddl\n"
          ]
        }
      ],
      "source": [
        "# Import the NLTK and the Porter Stemmer library\n",
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "p_stemmer = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlQb1Yh3mLml",
        "outputId": "7571197e-8d32-4125-b84a-ae186c661342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "house --> hous\n",
            "cat --> cat\n",
            "caterpillar --> caterpillar\n",
            "hungry --> hungri\n",
            "fiddle --> fiddl\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# The Snowball Stemmer requires that you pass a language parameter\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhEcsipFr4uY"
      },
      "source": [
        "#### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ4N9ffLmWTJ"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzNDRViGmbWt",
        "outputId": "a9e4d2c9-ba5d-49d7-871f-e1fe3952e1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "saw          VERB   11925638236994514241   see\n",
            "many         ADJ    9720044723474553187    many\n",
            "bikes        NOUN   16029548483725639901   bike\n",
            "on           ADP    5640369432778651323    on\n",
            "the          DET    7425985699627899538    the\n",
            "road         NOUN   13540101588783668053   road\n",
            "the          DET    7425985699627899538    the\n",
            "other        ADJ    1176656782636220709    other\n",
            "day          NOUN   1608482186128794349    day\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u\"I saw many bikes on the road the other day\")\n",
        "\n",
        "show_lemmas(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uRTa6PUtkEd"
      },
      "source": [
        "### Example on a Larger Data Set - Bag-of-Words Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ijzHZlqm95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "8043eb38-3d46-408e-f144-c61c94682ff9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cea7707c-1f8d-45d5-baf3-5c28a9b42805\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cea7707c-1f8d-45d5-baf3-5c28a9b42805\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'movie_lines.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ffc371de1de9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movie_lines.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movie_conversations.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movie_lines.txt'"
          ]
        }
      ],
      "source": [
        "import re # to preprocess the text\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "lines = open('movie_lines.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding='utf-8',errors='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciy73Pvbtrkp"
      },
      "outputs": [],
      "source": [
        "# we can check the structure of the lists - lines, conversations\n",
        "lines\n",
        "\n",
        "# We can see the lines list has index column\n",
        "# the conversations list has a vector with references to those index values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9YkS-bFtvFb"
      },
      "outputs": [],
      "source": [
        "# we need to construct a python dictionary\n",
        "# to map each line index to lines\n",
        "\n",
        "# We need to a dataset that maps inputs to outputs - supervised ML datasets\n",
        "\n",
        "# we also need to conduct several data cleaning steps\n",
        "\n",
        "id_to_line = {} # initialize the dictionary\n",
        "\n",
        "for line in lines:\n",
        "  _line = line.split(' +++$+++ ')\n",
        "  if len(_line) == 5:\n",
        "    id_to_line[_line[0]] = _line[4] # creates the dictionary entry\n",
        "\n",
        "id_to_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HxBhGcGtvCx"
      },
      "outputs": [],
      "source": [
        "# create a list with the line ids from the conversations list\n",
        "\n",
        "conversations_ids = []\n",
        "\n",
        "for conversation in conversations[:-1]:\n",
        "\n",
        "  # we need to clean up the data, remove the quotes, square brackets, white space etc.\n",
        "  _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "  # this returns the last element and removes the square brackets\n",
        "  # removes the quotes and whiate spaces\n",
        "  conversations_ids.append(_conversation.split(\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yzutBrNtu_-"
      },
      "outputs": [],
      "source": [
        "# Develop inputs and outputs to create a supervised ML dataset\n",
        "\n",
        "# in our conversation_ids dataset, the first element is the question\n",
        "# the second element is the answer\n",
        "\n",
        "# we create two lists, one for questions, one for answers...\n",
        "# we need to map our dictionary keys to our lines\n",
        "\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conversation in conversations_ids:\n",
        "  for i in range(len(conversation) - 1):\n",
        "    questions.append(id_to_line[conversation[i]])\n",
        "    answers.append(id_to_line[conversation[i+1]]) # the next element in the row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GCSVz2fxd8m"
      },
      "source": [
        "Please note this next cell ***may return offensive language*** due to the nature of the dataset used.  This is a widely used dataset in the field of Natural Language Processing.\n",
        "\n",
        "Simply run the cells to generate an alternative random sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as7ZwNo-u9-8"
      },
      "outputs": [],
      "source": [
        "# generate some random sentences from our list\n",
        "\n",
        "# Please note this cell returns a random list from a large dataset\n",
        "# of actual movie scripts, that is widely used in teaching NLP.\n",
        "\n",
        "# Therefore there is a chance your random generation\n",
        "# may return swear words or offensive language.\n",
        "\n",
        "# We run this to show you what the data looks like before it is processed\n",
        "\n",
        "from random import randrange\n",
        "x = (randrange(len(questions)))\n",
        "questions[x:x+5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EVQGRJotu9K"
      },
      "outputs": [],
      "source": [
        "# we need to clean up the data, expanding the abbreviations etc.\n",
        "# remove apostrophes, removing duplication, unimportant words\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower() # lower case\n",
        "  text = re.sub(r\"i'm\",\"i am\", text) # use re library to replace apostrophes\n",
        "  text = re.sub(r\"he's\",\"he is\", text)\n",
        "  text = re.sub(r\"she's\",\"she is\", text)\n",
        "  text = re.sub(r\"that's\",\"that is\", text)\n",
        "  text = re.sub(r\"what's\",\"what is\", text)\n",
        "  text = re.sub(r\"where's\",\"where is\", text)\n",
        "  text = re.sub(r\"\\'ll\",\" will\", text)\n",
        "  text = re.sub(r\"\\'ve\",\" have\", text)\n",
        "  text = re.sub(r\"\\'re\",\" are\", text)\n",
        "  text = re.sub(r\"\\'d\",\" would\", text)\n",
        "  text = re.sub(r\"won't\",\"will not\", text)\n",
        "  text = re.sub(r\"can't\",\"can not\", text)\n",
        "  text = re.sub(r\"[^a-zA-Z0-9]+\",\" \", text) # remove special characters\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLJcxwSauSnj"
      },
      "outputs": [],
      "source": [
        "# Clean up the questions\n",
        "clean_questions = []\n",
        "\n",
        "for question in questions:\n",
        "  clean_questions.append(clean_text(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sr-xqKHtu6C"
      },
      "outputs": [],
      "source": [
        "# Clean up the answers\n",
        "clean_answers = []\n",
        "\n",
        "for answer in answers:\n",
        "  clean_answers.append(clean_text(answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAopgs3Jtu3A"
      },
      "outputs": [],
      "source": [
        "# check our clean lists\n",
        "\n",
        "# notice the difference before and after.\n",
        "\n",
        "print(questions[x:x+2])\n",
        "print(clean_questions[x:x+2])\n",
        "print(\"---------------------------------\")\n",
        "print(answers[x:x+1])\n",
        "print(clean_answers[x:x+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkss1b9Pyed1"
      },
      "outputs": [],
      "source": [
        "# the next thing we do is remove the infrequent and unneccass ry words\n",
        "# we don't need to slow the training down for very few words\n",
        "\n",
        "# We want to optimise the vocab\n",
        "# we should focus on the most commonly used words in the corpus\n",
        "\n",
        "# create a dictionary - word distribution histogram\n",
        "\n",
        "word_hist = {}\n",
        "\n",
        "for question in clean_questions:\n",
        "  for word in question.split(): # do we see the word the first time?\n",
        "    if word not in word_hist:\n",
        "      word_hist[word] = 1\n",
        "    else:\n",
        "      word_hist[word] += 1\n",
        "\n",
        "for answer in clean_answers:\n",
        "  for word in answer.split(): # do we see the word the first time?\n",
        "    if word not in word_hist:\n",
        "      word_hist[word] = 1\n",
        "    else:\n",
        "      word_hist[word] += 1 # add it to existing count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLa2HHgl0ZJf"
      },
      "outputs": [],
      "source": [
        "# how many words do we have in our corpus?\n",
        "\n",
        "len(word_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwruwNae0cNF"
      },
      "outputs": [],
      "source": [
        "# We are now going to perform tokenization\n",
        "\n",
        "# then filter the words by a certain threshold, so we can exclude infrequent words\n",
        "\n",
        "threshold = 20   # any word occuring less than this, drop...\n",
        "\n",
        "question_words_to_int = {} # dictionary of words to integers\n",
        "\n",
        "word_number = 0  # this is our index, set of unique integers\n",
        "\n",
        "for word, count in word_hist.items():\n",
        "  if count >= threshold:\n",
        "    question_words_to_int[word] = word_number\n",
        "    word_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eOncXeO0eLb"
      },
      "outputs": [],
      "source": [
        "answer_words_to_int = {} # dictionary of words to integers\n",
        "\n",
        "word_number = 0  # this is our index, set of unique integers\n",
        "\n",
        "for word, count in word_hist.items():\n",
        "  if count >= threshold:\n",
        "    answer_words_to_int[word] = word_number\n",
        "    word_number += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-z98A3c0paH"
      },
      "source": [
        "We now have a dictionary (vector) with a unique integer assigned to each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrCIUhrs0lAW"
      },
      "outputs": [],
      "source": [
        "# Add the tokens for the encoder / decoder\n",
        "\n",
        "# SOS & EOS tokens\n",
        "# we can replace the infrequent word with tokens = OUT\n",
        "\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>','<SOS>']\n",
        "\n",
        "# add to our dictionaries\n",
        "\n",
        "for token in tokens:\n",
        "  question_words_to_int[token] = len(question_words_to_int) + 1\n",
        "\n",
        "for token in tokens:\n",
        " answer_words_to_int[token] = len(answer_words_to_int) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiy0e0wr0vK3"
      },
      "outputs": [],
      "source": [
        "# We create the inverse dictionary for the answer_words_to_int\n",
        "\n",
        "# we need this inverse mapping for the Seq2Seq model\n",
        "# dictionary inversing is an important step in many ML models\n",
        "\n",
        "inverse_ans_int_to_word = {w_i: w for w, w_i in answer_words_to_int.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBIsutXH0zN-"
      },
      "outputs": [],
      "source": [
        "# we need to add the EOS token to signal end of every answer for the decoder\n",
        "# loop over our clean answer and add EOS token\n",
        "\n",
        "for i in range(len(clean_answers)):\n",
        "  clean_answers[i] += ' <EOS>' # add a space\n",
        "\n",
        "\n",
        "# we have to specify the EOS for the model to work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SebL48E02k-"
      },
      "outputs": [],
      "source": [
        "# check that this has been added...\n",
        "\n",
        "clean_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni46NKLb07Kz"
      },
      "source": [
        "We no need to translate all our questions and answers into integers.\n",
        "\n",
        "We also need to replace all the words that we filtered out (below the threshold) with the 'OUT' token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4aQxV9b092-"
      },
      "outputs": [],
      "source": [
        "# We map our clean lists to the dictionary of integers\n",
        "# to create numerical vectors\n",
        "\n",
        "# replace filtered words with the <OUT> token\n",
        "\n",
        "questions_to_integers = []\n",
        "\n",
        "for question in clean_questions:\n",
        "  ints = []\n",
        "  for word in question.split():\n",
        "    if word not in question_words_to_int:\n",
        "      ints.append(question_words_to_int['<OUT>'])\n",
        "    else:\n",
        "      ints.append(question_words_to_int[word])\n",
        "  questions_to_integers.append(ints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a5I3wub1DKZ"
      },
      "outputs": [],
      "source": [
        "answers_to_integers = []\n",
        "\n",
        "for answer in clean_answers:\n",
        "  ints = []\n",
        "  for word in answer.split():\n",
        "    if word not in answer_words_to_int:\n",
        "      ints.append(answer_words_to_int['<OUT>'])\n",
        "    else:\n",
        "      ints.append(answer_words_to_int[word])\n",
        "  answers_to_integers.append(ints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET4YCq6Z1HS4"
      },
      "outputs": [],
      "source": [
        "# We now need to sort the questions and answers by\n",
        "# the length of the questions.  This speeds up the training\n",
        "# and reduces loss - by reducing the padding\n",
        "\n",
        "# We make a new list - we can's sort an existing list in python\n",
        "\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "\n",
        "# we don't want to include long sentences - say a length of 20 words in a sentence\n",
        "# this is a hyperparameter\n",
        "\n",
        "for length in range(1, 25 + 1):# 25 words in longest sentence\n",
        "   for i in enumerate(questions_to_integers):\n",
        "     if len(i[1]) == length:\n",
        "       sorted_clean_questions.append(questions_to_integers[i[0]])\n",
        "       sorted_clean_answers.append(answers_to_integers[i[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAqxNPjK1LmQ"
      },
      "outputs": [],
      "source": [
        "sorted_clean_questions[x:x+2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pnqlIFJkkJSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}